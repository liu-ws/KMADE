import torch.nn as nn
import os
import torch
from .kmades import SGKMADE, CSGKMADE, MGKMADE, CMGKMADE
from .saving import save_expr

"""
Implements a Masked Autoregressive Flow, which is a stack of mades such that the random numbers which drive made i
are generated by made i-1. The first made is driven by standard gaussian noise. In the current implementation, all
mades are of the same type. If there is only one made in the stack, then it's equivalent to a single made.
"""


class SGKMAF(nn.Module):
    def __init__(
        self,
        data_l: int,
        hidden_layers: list = [],
        n_mades: int = 1,
        batch_norm: bool = True,
        input_order: "str | list" = "sequential",
        mode: str = "sequential",
        **kwargs,
    ):
        """
        Masked Autoregressive Flow with SGKMADE blocks.

        Args:
            data_l : int
                Dimension of data.
            hidden_layers : list, optional
                Number of hidden units for each hidden layer. Default: [].
            n_mades : int, optional
                Number of MADEs. Default: 1.
            batch_norm : bool, optional
                Whether to use batch normalization between MADEs. Default: True.
            input_order : str or list, optional
                Order of inputs of last MADE. Default: "sequential".
            mode : str, optional
                Strategy for assigning degrees to hidden nodes: "random" or "sequential". Default: "sequential".
            **kwargs : Other keyword arguments for SGKMADE.
        """
        super().__init__()
        # save input arguments
        self.data_l = data_l
        self.hidden_layers = hidden_layers
        self.n_mades = n_mades

        self.input_order = input_order
        self.mode = mode

        self.device = kwargs.get("device", "cuda")

        self.mades = nn.ModuleList()
        self.batch_norm = batch_norm
        if batch_norm:
            self.bns = nn.ModuleList(
                [nn.BatchNorm1d(data_l).to(self.device) for _ in range(n_mades)]
            )

        self.logdet_dudx = 0.0

        ckpt_path = kwargs.get("ckpt_path", None)
        for i in range(n_mades):

            # create a new made

            if ckpt_path is not None:
                kwargs["ckpt_path"] = os.path.join(ckpt_path, f"made{i+1}")

            made = SGKMADE(
                data_l=data_l,
                hidden_layers=hidden_layers,
                input_order=input_order,
                mode=mode,
                **kwargs,
            )
            self.mades.append(made)
            input_order = (
                input_order
                if input_order == "random"
                else made.degrees[0].detach().cpu().tolist()[::-1]
            )

        self.input_order = self.mades[0].degrees[0]

    def forward(self, x, log=True, update_grid=False):
        """
        Forward pass through the MAF.

        Args:
            x (torch.Tensor): Input data.
            log (bool, optional): Whether to return log-likelihood. Default: True.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            tuple: (u, log-likelihood or likelihood)
        """
        self.logdet_dudx = 0.0

        for i in range(self.n_mades):

            if update_grid:
                self.mades[i].update_grid(x)

            x, m, logp = self.mades[i].compute_u(x)

            self.logdet_dudx += 0.5 * torch.sum(logp, axis=1)

            # batch normalization
            if self.batch_norm:
                bn = self.bns[i]
                x = bn(x)
                self.logdet_dudx += torch.sum(torch.log(bn.weight)) - 0.5 * torch.sum(
                    torch.log(bn.running_var + bn.eps)
                )

        # log likelihoods
        L = (
            -0.5 * self.data_l * torch.log(torch.tensor(2 * torch.pi))
            - 0.5 * torch.sum(x**2, axis=1)
            + self.logdet_dudx
        )

        return x, L if log else torch.exp(L)

    def loss_fn(self, x, update_grid=False):
        """
        Compute the negative log-likelihood loss.

        Args:
            x : torch.Tensor
                Input data.
            update_grid : bool, optional
                Whether to update grid. Default: False.

        Returns:
            torch.Tensor: Loss value.
        """
        return -torch.mean(self.forward(x, update_grid=update_grid)[1])

    def sample(self, n_samples=1, u=None):
        """
        Generate samples by propagating random numbers through each MADE.

        Args:
            n_samples : int, optional
                Number of samples. Default: 1.
            u : torch.Tensor, optional
                Random numbers to use in generating samples. If None, new random numbers are drawn.

        Returns:
            torch.Tensor: Generated samples.
        """
        x = (torch.randn(n_samples, self.data_l) if u is None else u).to(self.device)

        if self.batch_norm:
            # Reverse through the MADEs and batch norms
            for i in range(self.n_mades - 1, -1, -1):
                made = self.mades[i]
                # Get the corresponding batch norm layer
                bn = self.bns[i]
                if bn is not None:
                    x = (x - bn.bias) * torch.sqrt(
                        bn.running_var + bn.eps
                    ) / bn.weight + bn.running_mean
                x = made.sample(n_samples, u=x)
        else:
            # Just reverse through the MADEs
            for made in reversed(self.mades):
                x = made.sample(n_samples, u=x)

        return x

    def expr_fit(self):
        """
        Fit symbolic expressions for each MADE in the stack.
        """
        for i in range(self.n_mades):
            print(f"fit expressions for made{i+1}:")
            self.mades[i].auto_symbolic()

    def expr_save(self, path):
        """
        Save symbolic expressions for each MADE in the stack.

        Args:
            path (str): Directory to save expressions.
        """
        for i in range(self.n_mades):
            save_expr(path=os.path.join(path, f"made{i+1}"), model=self.mades[i])

    def saveckpt(self, path):
        """
        Save checkpoints for each MADE in the stack.

        Args:
            path (str): Directory to save checkpoints.
        """
        for i in range(self.n_mades):
            self.mades[i].saveckpt(os.path.join(path, f"made{i+1}"))


class CSGKMAF(nn.Module):
    def __init__(
        self,
        data_l: int,
        para_l: int,
        hidden_layers: list = [],
        para_hidden_layers: list = [],
        n_mades: int = 1,
        batch_norm: bool = True,
        input_order: "str | list" = "sequential",
        mode: str = "sequential",
        **kwargs,
    ):
        """
        Conditional Masked Autoregressive Flow with CSGKMADE blocks.

        Args:
            data_l : int
                Dimension of data.
            para_l : int
                Dimension of parameter.
            hidden_layers : list, optional
                Number of hidden units for each hidden layer. Default: [].
            para_hidden_layers : list, optional
                Number of hidden units for each parameter hidden layer. Default: [].
            n_mades : int, optional
                Number of MADEs. Default: 1.
            batch_norm : bool, optional
                Whether to use batch normalization between MADEs. Default: True.
            input_order : str or list, optional
                Order of inputs of last MADE. Default: "sequential".
            mode : str, optional
                Strategy for assigning degrees to hidden nodes: "random" or "sequential". Default: "sequential".
            **kwargs :
                Other keyword arguments for CSGKMADE.
        """
        super().__init__()
        # save input arguments
        self.data_l = data_l
        self.para_l = para_l
        self.hidden_layers = hidden_layers
        self.para_hidden_layers = para_hidden_layers
        self.n_mades = n_mades

        self.input_order = input_order
        self.mode = mode
        self.device = kwargs.get("device", "cuda")

        self.batch_norm = batch_norm
        if batch_norm:
            self.bns = nn.ModuleList(
                [nn.BatchNorm1d(data_l).to(self.device) for _ in range(n_mades)]
            )
        self.mades = nn.ModuleList()

        # jacobian
        self.logdet_dudx = 0.0
        ckpt_path = kwargs.get("ckpt_path", None)
        for i in range(n_mades):

            # create a new made
            if ckpt_path is not None:
                kwargs["ckpt_path"] = os.path.join(ckpt_path, f"made{i+1}")

            made = CSGKMADE(
                data_l=data_l,
                para_l=para_l,
                hidden_layers=hidden_layers,
                para_hidden_layers=para_hidden_layers,
                input_order=input_order,
                mode=mode,
                **kwargs,
            )
            self.mades.append(made)
            input_order = (
                input_order
                if input_order == "random"
                else made.degrees[0].detach().cpu().tolist()[::-1]
            )

        self.input_order = self.mades[0].degrees[0]

    def forward(self, x, log=True, update_grid=False):
        """
        Forward pass through the conditional MAF.

        Args:
            x (torch.Tensor): Input data.
            log (bool, optional): Whether to return log-likelihood. Default: True.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            tuple: (u, log-likelihood or likelihood)
        """
        self.logdet_dudx = 0.0

        for i in range(self.n_mades):

            if update_grid:
                self.mades[i].update_grid(x)

            u, m, logp = self.mades[i].compute_u(x)
            x[:, 0 : self.data_l] = u

            self.logdet_dudx += 0.5 * torch.sum(logp, axis=1)

            # batch normalization
            if self.batch_norm:
                bn = self.bns[i]
                x = bn(x)
                self.logdet_dudx += torch.sum(torch.log(bn.weight)) - 0.5 * torch.sum(
                    torch.log(bn.running_var + bn.eps)
                )

        # log likelihoods
        L = (
            -0.5 * self.data_l * torch.log(torch.tensor(2 * torch.pi))
            - 0.5 * torch.sum(u**2, axis=1)
            + self.logdet_dudx
        )

        return u, L if log else torch.exp(L)

    def loss_fn(self, x, update_grid=False):
        """
        Compute the negative log-likelihood loss.

        Args:
            x : torch.Tensor
                Input data.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            torch.Tensor: Loss value.
        """
        return -torch.mean(self.forward(x, update_grid=update_grid)[1])

    def sample(self, n_samples=1, u=None, para=[]):
        """
        Generate samples by propagating random numbers through each MADE.

        Args:
            n_samples (int, optional): Number of samples. Default: 1.
            u (torch.Tensor, optional): Random numbers to use in generating samples. If None, new random numbers are drawn.
            para (list, optional): Parameter values for conditional models. Default: [].

        Returns:
            torch.Tensor: Generated samples.
        """
        x = (torch.randn(n_samples, self.data_l) if u is None else u).to(self.device)
        if self.batch_norm:
            # Reverse through the MADEs and batch norms
            for i in range(self.n_mades - 1, -1, -1):
                made = self.mades[i]
                # Get the corresponding batch norm layer
                bn = self.bns[i]
                if bn is not None:
                    x = (x - bn.bias) * torch.sqrt(
                        bn.running_var + bn.eps
                    ) / bn.weight + bn.running_mean
                x = made.sample(n_samples, u=x, para=para)
        else:
            # Just reverse through the MADEs
            for made in reversed(self.mades):
                x = made.sample(n_samples, u=x, para=para)

        return x

    def auto_symbolic(self):
        """
        Fit symbolic expressions for each MADE in the stack.
        """
        for i in range(self.n_mades):
            print(f"auto_symbolic of made{i+1}:")
            self.mades[i].auto_symbolic()

    def expr_save(self, path):
        """
        Save symbolic expressions for each MADE in the stack.

        Args:
            path (str): Directory to save expressions.
        """
        for i in range(self.n_mades):
            save_expr(path=os.path.join(path, f"made{i+1}"), model=self.mades[i])

    def saveckpt(self, path):
        """
        Save checkpoints for each MADE in the stack.

        Args:
            path (str): Directory to save checkpoints.
        """
        for i in range(self.n_mades):
            self.mades[i].saveckpt(os.path.join(path, f"made{i+1}"))


class MGKMAF(nn.Module):
    def __init__(
        self,
        data_l: int,
        hidden_layers: list,
        n_comps: int,
        n_mades: int,
        batch_norm: bool = True,
        input_order: "str | list" = "sequential",
        mode: str = "sequential",
        **kwargs,
    ):
        """
        Masked Autoregressive Flow with MGKMADE as the last block.

        Args:
            data_l : int
                Dimension of data.
            hidden_layers : list
                Number of hidden units for each hidden layer.
            n_comps : int
                Number of Gaussians per conditional.
            n_mades : int
                Number of MADEs.
            batch_norm : bool, optional
                Whether to use batch normalization between MADEs. Default: True.
            input_order : str or list, optional
                Order of inputs of last MADE. Default: "sequential".
            mode : str, optional
                Strategy for assigning degrees to hidden nodes: "random" or "sequential". Default: "sequential".
            **kwargs : Other keyword arguments for MGKMADE.
        """
        super().__init__()
        # save input arguments
        self.data_l = data_l
        self.hidden_layers = hidden_layers
        self.n_comps = n_comps
        self.mode = mode
        self.n_mades = n_mades

        self.device = kwargs.get("device", "cuda")

        self.batch_norm = batch_norm
        self.mades = nn.ModuleList()

        # maf
        self.maf = SGKMAF(
            n_mades=n_mades - 1,
            data_l=data_l,
            hidden_layers=hidden_layers,
            batch_norm=batch_norm,
            input_order=input_order,
            mode=mode,
            **kwargs,
        )
        self.mades.extend(self.maf.mades)

        if batch_norm:
            self.bns = self.maf.bns
        self.input_order = self.maf.input_order

        # mog made
        input_order = (
            input_order
            if input_order == "random"
            else self.maf.mades[-1].degrees[0].detach().cpu().tolist()[::-1]
        )

        ckpt_path = kwargs.get("ckpt_path", None)
        if ckpt_path is not None:
            kwargs["ckpt_path"] = os.path.join(ckpt_path, f"made{n_mades}")

        self.made = MGKMADE(
            data_l=data_l,
            hidden_layers=hidden_layers,
            n_comps=n_comps,
            input_order=input_order,
            mode=mode,
            **kwargs,
        )
        self.mades.append(self.made)

    def forward(self, x, log=True, update_grid=False):
        """
        Forward pass through the MAF.

        Args:
            x (torch.Tensor): Input data.
            log (bool, optional): Whether to return log-likelihood. Default: True.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            tuple: (u, log-likelihood or likelihood)
        """
        # log likelihoods
        u, _ = self.maf.forward(x, log=True, update_grid=update_grid)

        if update_grid:
            self.made.update_grid(u)

        L = self.made.likelihood(u, log=True) + self.maf.logdet_dudx
        u = self.made.compute_u(u)[0]
        return u, L if log else torch.exp(L)

    def loss_fn(self, x, update_grid=False):
        """
        Compute the negative log-likelihood loss.

        Args:
            x (torch.Tensor): Input data.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            torch.Tensor: Loss value.
        """
        return -torch.mean(self.forward(x, update_grid=update_grid)[1])

    def sample(self, n_samples=1, u=None):
        """
        Generate samples by propagating random numbers through each MADE.

        Args:
            n_samples (int, optional): Number of samples. Default: 1.
            u (torch.Tensor, optional): Random numbers to use in generating samples. If None, new random numbers are drawn.

        Returns:
            torch.Tensor: Generated samples.
        """
        x = (torch.randn(n_samples, self.data_l) if u is None else u).to(self.device)
        x = self.made.sample(n_samples, u=x)
        x = self.maf.sample(n_samples, u=x)

        return x

    def auto_symbolic(self):
        """
        Fit symbolic expressions for each MADE in the stack.
        """
        for i in range(self.n_mades):
            print(f"auto_symbolic of made{i+1}:")
            self.mades[i].auto_symbolic()

    def expr_save(self, path):
        """
        Save symbolic expressions for each MADE in the stack.

        Args:
            path (str): Directory to save expressions.
        """
        for i in range(self.n_mades):
            save_expr(path=os.path.join(path, f"made{i+1}"), model=self.mades[i])

    def saveckpt(self, path):
        """
        Save checkpoints for each MADE in the stack.

        Args:
            path (str): Directory to save checkpoints.
        """
        for i in range(self.n_mades):
            self.mades[i].saveckpt(os.path.join(path, f"made{i+1}"))


class CMGKMAF(nn.Module):
    def __init__(
        self,
        data_l: int,
        hidden_layers: list,
        para_hidden_layers: list,
        n_comps: int,
        n_mades: int,
        batch_norm: bool = True,
        input_order: "str | list" = "sequential",
        mode: str = "sequential",
        **kwargs,
    ):
        """
        Conditional Masked Autoregressive Flow with CMGKMADE as the last block.

        Args:
            data_l (int): Dimension of data.
            hidden_layers (list): Number of hidden units for each hidden layer.
            para_hidden_layers (list): Number of hidden units for each parameter hidden layer.
            n_comps (int): Number of Gaussians per conditional.
            n_mades (int): Number of MADEs.
            batch_norm (bool, optional): Whether to use batch normalization between MADEs. Default: True.
            input_order (str or list, optional): Order of inputs of last MADE. Default: "sequential".
            mode (str, optional): Strategy for assigning degrees to hidden nodes: "random" or "sequential". Default: "sequential".
            **kwargs: Other keyword arguments for CMGKMADE.
        """
        super().__init__()
        # save input arguments
        self.data_l = data_l
        self.hidden_layers = hidden_layers
        self.para_hidden_layers = para_hidden_layers
        self.n_comps = n_comps
        self.batch_norm = batch_norm
        self.mode = mode
        self.n_mades = n_mades
        self.mades = nn.ModuleList()
        self.device = kwargs.get("device", "cuda")
        # maf
        self.maf = CSGKMAF(
            n_mades=n_mades - 1,
            data_l=data_l,
            hidden_layers=hidden_layers,
            para_hidden_layers=para_hidden_layers,
            batch_norm=batch_norm,
            input_order=input_order,
            mode=mode,
            **kwargs,
        )
        self.mades.extend(self.maf.mades)
        if batch_norm:
            self.bns = self.maf.bns
        self.input_order = self.maf.input_order

        # mog made
        input_order = (
            input_order
            if input_order == "random"
            else self.maf.mades[-1].degrees[0].detach().cpu().tolist()[::-1]
        )

        ckpt_path = kwargs.get("ckpt_path", None)
        if ckpt_path is not None:
            kwargs["ckpt_path"] = os.path.join(ckpt_path, f"made{n_mades}")

        self.made = CMGKMADE(
            data_l=data_l,
            hidden_layers=hidden_layers,
            para_hidden_layers=para_hidden_layers,
            n_comps=n_comps,
            input_order=input_order,
            mode=mode,
            **kwargs,
        )
        self.mades.append(self.made)

    def forward(self, x, log=True, update_grid=False):
        """
        Forward pass through the conditional MAF.

        Args:
            x (torch.Tensor): Input data.
            log (bool, optional): Whether to return log-likelihood. Default: True.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            tuple: (u, log-likelihood or likelihood)
        """
        # log likelihoods
        u, _ = self.maf.forward(x, log=True, update_grid=update_grid)
        x[:, 0 : self.data_l] = u

        if update_grid:
            self.made.update_grid(u)

        L = self.made.likelihood(x, log=True) + self.maf.logdet_dudx
        u = self.made.compute_u(x)[0]
        return u, L if log else torch.exp(L)

    def loss_fn(self, x, update_grid=False):
        """
        Compute the negative log-likelihood loss.

        Args:
            x (torch.Tensor): Input data.
            update_grid (bool, optional): Whether to update grid. Default: False.

        Returns:
            torch.Tensor: Loss value.
        """
        return -torch.mean(self.forward(x, update_grid=update_grid)[1])

    def sample(self, n_samples=1, u=None, para=[]):
        """
        Generate samples by propagating random numbers through each MADE.

        Args:
            n_samples (int, optional): Number of samples. Default: 1.
            u (torch.Tensor, optional): Random numbers to use in generating samples. If None, new random numbers are drawn.
            para (list, optional): Parameter values for conditional models. Default: [].

        Returns:
            torch.Tensor: Generated samples.
        """
        x = (torch.randn(n_samples, self.data_l) if u is None else u).to(self.device)
        x = self.made.sample(n_samples, u=x, para=para)
        x = self.maf.sample(n_samples, u=x, para=para)

        return x

    def auto_symbolic(self):
        """
        Fit symbolic expressions for each MADE in the stack.
        """
        for i in range(self.n_mades):
            print(f"auto_symbolic of made{i+1}:")
            self.mades[i].auto_symbolic()

    def expr_save(self, path):
        """
        Save symbolic expressions for each MADE in the stack.

        Args:
            path (str): Directory to save expressions.
        """
        for i in range(self.n_mades):
            save_expr(path=os.path.join(path, f"made{i+1}"), model=self.mades[i])

    def saveckpt(self, path):
        """
        Save checkpoints for each MADE in the stack.

        Args:
            path (str): Directory to save checkpoints.
        """
        for i in range(self.n_mades):
            self.mades[i].saveckpt(os.path.join(path, f"made{i+1}"))
